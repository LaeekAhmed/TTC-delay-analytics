{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6714fb3",
   "metadata": {},
   "source": [
    "## 1. Spark setup ðŸŒ \n",
    "\n",
    "we will import the required libraries and setup the configs required to start the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05c4e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n",
      "/home/shaikh/spark/spark-3.3.3-bin-hadoop3/python/pyspark/__init__.py\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: this scirpt is a driver program which creates the SparkContext \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "print(pyspark.__version__)\n",
    "print(pyspark.__file__)\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, when, round, format_number, initcap, concat, lit\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# remove side padding in notebook : https://stackoverflow.com/a/34058270/19268172\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02b81752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ttc-da-instance.us-east4-a.c.ttc-data-analytics.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_etl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2180077f10>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials_location = \"/home/shaikh/.google/credentials/ttc-data-analytics-key.json\"\n",
    "\n",
    "# Configure SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('spark_etl') \\\n",
    "    .config(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.11.jar, ./lib/spark-bigquery-with-dependencies_2.12-0.24.0.jar\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure Hadoop Configuration\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "spark\n",
    "# we stored the connector files in the lib/ dir :\n",
    "# !ls lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf1fc3",
   "metadata": {},
   "source": [
    "## 1.1 Understanding source data columns ðŸ“…\n",
    "\n",
    "\n",
    "> Subway\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Date|Date (YYYY/MM/DD)|12/31/2016|\n",
    "|Time|Time (24h clock)|1:59|\n",
    "|Day|Name of the day of the week|Saturday|\n",
    "|Station|TTC subway station name|Rosedale Station|\n",
    "|Code|TTC delay code|MUIS|\n",
    "|Min Delay|Delay (in minutes) to subway service|5|\n",
    "|Min Gap|Time length (in minutes) between trains|9|\n",
    "|Bound|Direction of train dependant on the line|N|\n",
    "|Line|TTC subway line i.e. YU, BD, SHP, and SRT|YU|\n",
    "|Vehicle|TTC train number|5961|\n",
    "\n",
    "we have a **delay code** lookup file which we can join with `Code` column \n",
    "\n",
    "we also have **line code** lookup values :  \n",
    "Line 1 Yonge-University (YU), Line 2 Bloor-Danforth (BD), Line 3 Scarborough (SRT), Line 4 Sheppard (SHP) which we can join with the `Line` column\n",
    "\n",
    "> Bus\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Report Date|The date (YYYY/MM/DD) when the delay-causing incident occurred|6/20/2017|\n",
    "|Route|The number of the bus route|51|\n",
    "|Time|Time (24h clock) when the delay-causing incident occurred|12:35:00 AM|\n",
    "|Day|The name of the day|Monday|\n",
    "|Location|The location of the delay-causing incident|York Mills Station|\n",
    "|Incident|The description of the delay-causing incident|Mechanical|\n",
    "|Min Delay|The delay, in minutes, to the schedule for the following bus|10|\n",
    "|Min Gap|The total scheduled time, in minutes, from the bus ahead of the following bus|20|\n",
    "|Direction|The direction of the bus route where B,b or BW indicates both ways. <br>(On an east west route, it includes both east and west)<br>NB - northbound, SB - southbound, EB - eastbound, WB - westbound|N||\n",
    "|Vehicle|Vehicle number|1057|\n",
    "\n",
    "> Streetcar\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Report Date|The date (YYYY/MM/DD) when the delay-causing incident occurred|6/20/2017|\n",
    "|Route|The number of the streetcar route|51|\n",
    "|Time|Time (24h clock) when the delay-causing incident occurred|12:35:00 AM|\n",
    "|Day|The name of the day|Monday|\n",
    "|Location|The location of the delay-causing incident|York Mills Station|\n",
    "|Incident|The description of the delay-causing incident|Mechanical|\n",
    "|Min Delay|The delay, in minutes, to the schedule for the following streetcar|10|\n",
    "|Min Gap|The total scheduled time, in minutes, from the streetcar ahead of the following streetcar|20|\n",
    "|Direction|The direction of the bus route where B,b or BW indicates both ways.<br>(On an east west route, it includes both east and west)<br>NB - northbound, SB - southbound, EB - eastbound, WB - westbound<br>The direction is not case sensitive|N|\n",
    "|Vehicle|Vehicle number|1057|\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71377b5",
   "metadata": {},
   "source": [
    "## 2. Extracting data from gcs ðŸ“¤\n",
    "\n",
    "This will be the first part of the ETL process - The **Extraction** (Reading) process\n",
    "\n",
    "to read data from gcs, we follow these steps : [read data from gcs using spark (from boslai's notes)](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week5.md#setup-to-read-from-gcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a4f3c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Route: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "\n",
      "Streetcar DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Line: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Bound: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "\n",
      "Subway DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Bound: string (nullable = true)\n",
      " |-- Line: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bus = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/bus_delay_data/*')\n",
    "df_subway = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/subway_delay_data/ttc-subway*')\n",
    "df_streetcar = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/streetcar_delay_data/*')\n",
    "\n",
    "print(\"Bus DataFrame Schema:\")\n",
    "df_bus.printSchema()\n",
    "\n",
    "print(\"\\nStreetcar DataFrame Schema:\")\n",
    "df_streetcar.printSchema()\n",
    "\n",
    "print(\"\\nSubway DataFrame Schema:\")\n",
    "df_subway.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab62352",
   "metadata": {},
   "source": [
    "## 3. TransformationsðŸ”§\n",
    "\n",
    "This is the 2nd step of the ETL process, the **Transformation** process. Now we will modify the extracted data so as to make it fit for being used as a table which will later be used to create dashboards.\n",
    "\n",
    "**1. Extra unwanted column**\n",
    "\n",
    "we have an extra column named `index_level_0` in your DataFrame that you didn't have in the original source `.xlsx` file.  \n",
    "This could be due to the index column being converted to a regular column during processing.  \n",
    "\n",
    "In pandas, an index is a special column that serves as a unique identifier for each row in a DataFrame or Series.\n",
    "By default, when you create a DataFrame in pandas, it automatically assigns a numeric index to each row, starting from 0 and incrementing by 1.\n",
    "\n",
    "we will remove this using `df_bus = df_bus.drop(\"__index_level_0__\")`\n",
    "\n",
    "---\n",
    "**2. converting `long` types to `int` as it occupies less memory**  \n",
    "\n",
    "we will use the `.cast(\"int\"))` method\n",
    "\n",
    "---\n",
    "\n",
    "**3. Transforming column values to lower case**\n",
    "\n",
    "The columns `Station` and `Location` have all values in Caps, so we have to change it  \n",
    "\n",
    "we also want to capitalize the first letter of each word in a column while converting the rest of the letters to lowercase, so we can use the `initcap()` method from `pyspark.sql.functions`. using ref : https://stackoverflow.com/a/68370448/19268172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71566fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Route: integer (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: integer (nullable = true)\n",
      " |-- Min Gap: integer (nullable = true)\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Vehicle: integer (nullable = true)\n",
      "\n",
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "|               Date|Route| Time|     Day|            Location|            Incident|Min Delay|Min Gap|Direction|Vehicle|\n",
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "|2022-01-01 00:00:00|  320|02:00|Saturday|    Yonge And Dundas|       General Delay|        0|      0|     null|   8531|\n",
      "|2022-01-01 00:00:00|  325|02:00|Saturday|Overlea And Thorc...|           Diversion|      131|    161|        W|   8658|\n",
      "|2022-01-01 00:00:00|  320|02:00|Saturday|   Yonge And Steeles|Operations - Oper...|       17|     20|        S|      0|\n",
      "|2022-01-01 00:00:00|  320|02:07|Saturday|   Yonge And Steeles|Operations - Oper...|        4|     11|        S|      0|\n",
      "|2022-01-01 00:00:00|  320|02:13|Saturday|   Yonge And Steeles|Operations - Oper...|        4|      8|        S|      0|\n",
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. dropping the index column\n",
    "df_bus = df_bus.drop(\"__index_level_0__\")\n",
    "df_streetcar = df_streetcar.drop(\"__index_level_0__\")\n",
    "\n",
    "# 2. Convert common Long Type columns to Integer Type\n",
    "for col_name in [\"Min Delay\", \"Min Gap\", \"Vehicle\"]:\n",
    "    df_bus = df_bus.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    df_subway = df_subway.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    df_streetcar = df_streetcar.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    # â†ª `col(col_name).cast(\"int\")` will be the new value for `col_name`\n",
    "\n",
    "# convert un-common Long Type columns to Integer Type\n",
    "df_streetcar = df_streetcar.withColumn(\"Line\", col(\"Line\").cast(\"int\"))\n",
    "df_bus = df_bus.withColumn(\"Route\", col(\"Route\").cast(\"int\"))\n",
    "\n",
    "# 3. Capitalize the first letter of each word in the \"Location\" column\n",
    "df_bus = df_bus.withColumn(\"Location\", initcap(\"Location\"))\n",
    "df_streetcar = df_streetcar.withColumn(\"Location\", initcap(\"Location\"))\n",
    "df_subway = df_subway.withColumn(\"Station\", initcap(\"Station\"))\n",
    "\n",
    "# check the results\n",
    "print(\"Bus DataFrame Schema:\")\n",
    "df_bus.printSchema()\n",
    "\n",
    "df_bus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5356f",
   "metadata": {},
   "source": [
    "## 3.1 Continuing Transformations ðŸ› ï¸\n",
    "\n",
    "3. we use `F.to_date()` built-in Spark function that converts a timestamp to date format (year, month and day only, no hour and minute). on `Date` column\n",
    "---\n",
    "4. we need to rename some columns before merging (union) so that we can distinguish between the types\n",
    "---\n",
    "5. We need to deal with column having `null` values\n",
    "---\n",
    "6. we need to create a union the 3 dfs to create one single df (ie. a table in BQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd893d05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident un-common values :  19\n",
      "Incident common values :  11\n",
      "\n",
      "Location un-common values :  14934\n",
      "Location common values :  625\n",
      "\n",
      "Bus DataFrame :\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "|      Date|Bus_Route| Time|     Day|            Location|            Incident|Min Delay|Min Gap|Direction|Bus_Number|\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "|2022-01-01|      320|02:00|Saturday|    Yonge And Dundas|       General Delay|        0|      0|     null|      8531|\n",
      "|2022-01-01|      325|02:00|Saturday|Overlea And Thorc...|           Diversion|      131|    161|        W|      8658|\n",
      "|2022-01-01|      320|02:00|Saturday|   Yonge And Steeles|Operations - Oper...|       17|     20|        S|         0|\n",
      "|2022-01-01|      320|02:07|Saturday|   Yonge And Steeles|Operations - Oper...|        4|     11|        S|         0|\n",
      "|2022-01-01|      320|02:13|Saturday|   Yonge And Steeles|Operations - Oper...|        4|      8|        S|         0|\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Streetcar DataFrame :\n",
      "+----------+------------+-----+--------+-------------------+--------------------+---------+-------+---------+-------------+\n",
      "|      Date|Strcar_Route| Time|     Day|           Location|            Incident|Min Delay|Min Gap|Direction|Strcar_Number|\n",
      "+----------+------------+-----+--------+-------------------+--------------------+---------+-------+---------+-------------+\n",
      "|2022-01-01|         504|02:21|Saturday|  Broadview Station|Collision - TTC I...|       30|     60|        E|         8333|\n",
      "|2022-01-01|         501|03:22|Saturday|  718 Queen St East|          Operations|       16|     35|        W|         8068|\n",
      "|2022-01-01|         504|03:28|Saturday|  Broadview Station|          Operations|       18|     36|        S|            0|\n",
      "|2022-01-01|         510|03:34|Saturday|      Union Station|          Operations|       30|     60|     null|         4406|\n",
      "|2022-01-01|         301|03:39|Saturday|Lakeshore And Tenth|            Security|        5|     25|        W|         8622|\n",
      "+----------+------------+-----+--------+-------------------+--------------------+---------+-------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Subway DataFrame :\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+---------+----+---------+\n",
      "|      Date| Time|     Day|             Station| Code|Min Delay|Min Gap|Direction|Line|Train_Num|\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+---------+----+---------+\n",
      "|2022-01-01|15:59|Saturday|Lawrence East Sta...| SRDP|        0|      0|        N| SRT|     3023|\n",
      "|2022-01-01|02:23|Saturday|  Spadina Bd Station| MUIS|        0|      0|     null|  BD|        0|\n",
      "|2022-01-01|22:00|Saturday|Kennedy Srt Stati...|  MRO|        0|      0|     null| SRT|        0|\n",
      "|2022-01-01|02:28|Saturday|  Vaughan Mc Station| MUIS|        0|      0|     null|  YU|        0|\n",
      "|2022-01-01|02:34|Saturday|    Eglinton Station|MUATC|        0|      0|        S|  YU|     5981|\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+---------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. modifying the DataFrame inside the loop won't affect the original DataFrames unless we reassign them.\n",
    "dataframes = [df_bus, df_streetcar, df_subway]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].withColumn('Date', F.to_date(dataframes[i].Date))\n",
    "\n",
    "# Re-assign the modified DataFrames to their original variable names\n",
    "df_bus, df_streetcar, df_subway = dataframes\n",
    "\n",
    "\n",
    "# 4. Rename some columns\n",
    "df_bus = df_bus \\\n",
    "    .withColumnRenamed('Route', 'Bus_Route') \\\n",
    "    .withColumnRenamed('Vehicle', 'Bus_Number')\n",
    "\n",
    "df_streetcar = df_streetcar \\\n",
    "    .withColumnRenamed('Line', 'Strcar_Route') \\\n",
    "    .withColumnRenamed('Bound', 'Direction') \\\n",
    "    .withColumnRenamed('Vehicle', 'Strcar_Number')\n",
    "\n",
    "df_subway = df_subway \\\n",
    "    .withColumnRenamed('Bound', 'Direction') \\\n",
    "    .withColumnRenamed('Vehicle', 'Train_Num')\n",
    "\n",
    "# check if the columns contain any un-common values\n",
    "\n",
    "count1 = df_bus.select(\"Incident\") \\\n",
    "    .exceptAll(df_streetcar.select(\"Incident\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "count2 = df_streetcar.select(\"Incident\") \\\n",
    "    .exceptAll(df_bus.select(\"Incident\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "print(\"Incident un-common values : \",count1+count2)\n",
    "\n",
    "print(\"Incident common values : \",df_bus.select(\"Incident\") \\\n",
    "    .intersect(df_streetcar.select(\"Incident\")) \\\n",
    "    .count())\n",
    "\n",
    "# same for column \"Location\"\n",
    "count1 = df_bus.select(\"Location\") \\\n",
    "    .exceptAll(df_streetcar.select(\"Location\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "count2 = df_streetcar.select(\"Location\") \\\n",
    "    .exceptAll(df_bus.select(\"Location\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "print(\"\\nLocation un-common values : \",count1+count2)\n",
    "\n",
    "print(\"Location common values : \",df_bus.select(\"Location\") \\\n",
    "    .intersect(df_streetcar.select(\"Location\")) \\\n",
    "    .count())\n",
    "\n",
    "# check the results\n",
    "print(\"\\nBus DataFrame :\")\n",
    "df_bus.show(5)\n",
    "\n",
    "print(\"\\nStreetcar DataFrame :\")\n",
    "df_streetcar.show(5)\n",
    "\n",
    "print(\"\\nSubway DataFrame :\")\n",
    "df_subway.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3263e57",
   "metadata": {},
   "source": [
    "## 3.2 Data Cleaning ðŸ§¹\n",
    "\n",
    "we have a `ttc-delay-code.parquet` lookup file which we can join with `Code` column `df_subway`. But, the content of this file has to be cleaned using various transformations and applying union! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4913865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|Delay_Code|   Code_Description|\n",
      "+----------+-------------------+\n",
      "|      EUAC|   Air Conditioning|\n",
      "|      EUAL|Alternating Current|\n",
      "|     EUATC| ATC RC&S Equipment|\n",
      "|      EUBK|             Brakes|\n",
      "|      EUBO|               Body|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zone_lookup = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/subway_delay_data/ttc-delay-code.parquet')\n",
    "# print(df_zone_lookup.count())\n",
    "# df_zone_lookup.show()\n",
    "\n",
    "# Drop the unwanted columns\n",
    "columns_to_drop = [\"Unnamed: 0\", \"Unnamed: 1\", \"Unnamed: 4\", \"Unnamed: 5\"]\n",
    "df_zone_lookup = df_zone_lookup.drop(*columns_to_drop)\n",
    "\n",
    "# Rename some columns\n",
    "df_subway = df_subway.withColumnRenamed(\"Delay Code\", \"Delay_Code\")\n",
    "\n",
    "df_zone_lookup = df_zone_lookup \\\n",
    "    .withColumnRenamed('Unnamed: 2', 'Delay_Code1') \\\n",
    "    .withColumnRenamed('Unnamed: 6', 'Delay_Code2') \\\n",
    "    .withColumnRenamed('Unnamed: 3', 'Code_Description1') \\\n",
    "    .withColumnRenamed('Unnamed: 7', 'Code_Description2') \\\n",
    "\n",
    "# Remove the first row\n",
    "df_zone_lookup = df_zone_lookup.filter(df_zone_lookup.Delay_Code1 != \"SUB RMENU CODE\")\n",
    "\n",
    "# merging duplicate columns by first seperate dfs for subway and srt and then apply union :\n",
    "\n",
    "# Select and rename columns using alias\n",
    "df_zone_lookup_sub = df_zone_lookup.select(\n",
    "    df_zone_lookup.Delay_Code1.alias(\"Delay_Code\"),\n",
    "    df_zone_lookup.Code_Description1.alias(\"Code_Description\")\n",
    ")\n",
    "\n",
    "# df_zone_lookup_sub.show()\n",
    "\n",
    "df_zone_lookup_srt = df_zone_lookup.select(\n",
    "    df_zone_lookup.Delay_Code2.alias(\"Delay_Code\"),\n",
    "    df_zone_lookup.Code_Description2.alias(\"Code_Description\")\n",
    ")\n",
    "\n",
    "# df_zone_lookup_srt.show()\n",
    "\n",
    "df_zone_lookup = df_zone_lookup_sub.unionAll(df_zone_lookup_srt).filter(col(\"Delay_Code\").isNotNull())\n",
    "\n",
    "# confirm results\n",
    "df_zone_lookup.show(5)\n",
    "df_zone_lookup.count() # 129 + 71 = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad3a525",
   "metadata": {},
   "source": [
    "## 3.3 Union and joins ðŸŽ‡\n",
    "\n",
    "Now that that we have cleaned `df_zone_lookup`, its time to join it with `df_subway` on the `Delay_Code`/`Code` columns\n",
    "\n",
    "we also have **line code** lookup values :  \n",
    "Line 1 Yonge-University (YU), Line 2 Bloor-Danforth (BD), Line 3 Scarborough (SRT), Line 4 Sheppard (SHP) which we can join with the `Line` column of `df_subway`\n",
    "\n",
    "common columns from all the 3 dfs to create one union df\n",
    "\n",
    "type column to refer the 3 types\n",
    "\n",
    "union of `df_bus` and `df_streetcar` only ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68aea80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+--------------------+---------+-------+---------+--------------------+---------+----------+--------------------+\n",
      "|      Date| Time|     Day|             Station|Min Delay|Min Gap|Direction|                Line|Train_Num|Delay_Code|    Code_Description|\n",
      "+----------+-----+--------+--------------------+---------+-------+---------+--------------------+---------+----------+--------------------+\n",
      "|2022-01-01|15:59|Saturday|Lawrence East Sta...|        0|      0|        N|   Scarborough (SRT)|     3023|      SRDP|   Disorderly Patron|\n",
      "|2022-01-01|02:23|Saturday|  Spadina Bd Station|        0|      0|     null| Bloor-Danforth (BD)|        0|      MUIS|Injured or ill Cu...|\n",
      "|2022-01-01|22:00|Saturday|Kennedy Srt Stati...|        0|      0|     null|   Scarborough (SRT)|        0|       MRO| Miscellaneous Other|\n",
      "|2022-01-01|02:28|Saturday|  Vaughan Mc Station|        0|      0|     null|Yonge-University ...|        0|      MUIS|Injured or ill Cu...|\n",
      "|2022-01-01|02:34|Saturday|    Eglinton Station|        0|      0|        S|Yonge-University ...|     5981|     MUATC|         ATC Project|\n",
      "+----------+-----+--------+--------------------+---------+-------+---------+--------------------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: joins df_subway with df_zone_lookup to get columns : Delay_Code, Code_Description:\n",
    "\n",
    "df_subway = df_subway.join(df_zone_lookup, df_subway.Code == df_zone_lookup.Delay_Code)\n",
    "# â†ª NOTE: use on=['col1'] if col1 is present in both dfs\n",
    "\n",
    "df_subway = df_subway.drop('Code')\n",
    "\n",
    "# replace Line codes with values :\n",
    "'''\n",
    "replace values of `Line` column of df_subway with :\n",
    "\n",
    "case \"YU\" : \"Yonge-University (YU)\"\n",
    "case \"BD\" : \"Bloor-Danforth (BD)\"\n",
    "case \"SRT\" : \"Scarborough (SRT)\"\n",
    "case \"SHP\" : \"Sheppard (SHP)\"\n",
    "'''\n",
    "\n",
    "df_subway = df_subway.withColumn(\"Line\", \n",
    "                                 when(col(\"Line\") == \"YU\", \"Yonge-University (YU)\")\n",
    "                                 .when(col(\"Line\") == \"BD\", \"Bloor-Danforth (BD)\")\n",
    "                                 .when(col(\"Line\") == \"SRT\", \"Scarborough (SRT)\")\n",
    "                                 .when(col(\"Line\") == \"SHP\", \"Sheppard (SHP)\")\n",
    "                                 .otherwise(col(\"Line\")))\n",
    "\n",
    "df_subway.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7803d42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|service_type|count|\n",
      "+------------+-----+\n",
      "|         bus|78176|\n",
      "|   streetcar|23366|\n",
      "+------------+-----+\n",
      "\n",
      "+----------+-----+--------+--------------------+--------------------+---------+-------+---------+------------+\n",
      "|      Date| Time|     Day|            Location|            Incident|Min Delay|Min Gap|Direction|service_type|\n",
      "+----------+-----+--------+--------------------+--------------------+---------+-------+---------+------------+\n",
      "|2022-01-01|02:00|Saturday|    Yonge And Dundas|       General Delay|        0|      0|     null|         bus|\n",
      "|2022-01-01|02:00|Saturday|Overlea And Thorc...|           Diversion|      131|    161|        W|         bus|\n",
      "|2022-01-01|02:00|Saturday|   Yonge And Steeles|Operations - Oper...|       17|     20|        S|         bus|\n",
      "|2022-01-01|02:07|Saturday|   Yonge And Steeles|Operations - Oper...|        4|     11|        S|         bus|\n",
      "|2022-01-01|02:13|Saturday|   Yonge And Steeles|Operations - Oper...|        4|      8|        S|         bus|\n",
      "+----------+-----+--------+--------------------+--------------------+---------+-------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-union : Create the list of columns present in the two datasets\n",
    "common_colums = []\n",
    "bus_columns = set(df_bus.columns)\n",
    "\n",
    "for col in df_streetcar.columns:\n",
    "    if col in bus_columns:\n",
    "        common_colums.append(col)\n",
    "\n",
    "# Create a column `service_type` indicating where the data comes from.\n",
    "\n",
    "df_bus_sel = df_bus \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('bus'))\n",
    "\n",
    "df_streetcar_sel = df_streetcar \\\n",
    "    .select(common_colums) \\\n",
    "    .withColumn('service_type', F.lit('streetcar'))\n",
    "\n",
    "# Create a new DataFrame containing union of rows of green and yellow DataFrame.\n",
    "df_road_delay_data = df_bus_sel.unionAll(df_streetcar_sel)\n",
    "\n",
    "df_road_delay_data.groupBy('service_type').count().show()\n",
    "\n",
    "df_road_delay_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cf303d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+---------+-------+---------+------------+\n",
      "|      Date| Time|     Day|Min Delay|Min Gap|Direction|service_type|\n",
      "+----------+-----+--------+---------+-------+---------+------------+\n",
      "|2022-01-01|02:00|Saturday|        0|      0|     null|         bus|\n",
      "|2022-01-01|02:00|Saturday|      131|    161|        W|         bus|\n",
      "|2022-01-01|02:00|Saturday|       17|     20|        S|         bus|\n",
      "|2022-01-01|02:07|Saturday|        4|     11|        S|         bus|\n",
      "|2022-01-01|02:13|Saturday|        4|      8|        S|         bus|\n",
      "+----------+-----+--------+---------+-------+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of columns present in all three datasets\n",
    "common_columns = []\n",
    "bus_columns = set(df_bus.columns)\n",
    "streetcar_columns = set(df_streetcar.columns)\n",
    "\n",
    "for col in df_subway.columns:\n",
    "    if col in bus_columns and col in streetcar_columns:\n",
    "        common_columns.append(col)\n",
    "\n",
    "# Create columns with 'service_type' for each DataFrame\n",
    "df_bus_sel = df_bus.select(common_columns).withColumn('service_type', F.lit('bus'))\n",
    "df_streetcar_sel = df_streetcar.select(common_columns).withColumn('service_type', F.lit('streetcar'))\n",
    "df_subway_sel = df_subway.select(common_columns).withColumn('service_type', F.lit('subway'))\n",
    "\n",
    "# Union the three DataFrames\n",
    "df_all_delay_data = df_bus_sel.unionAll(df_streetcar_sel).unionAll(df_subway_sel)\n",
    "\n",
    "# Append \", Toronto\" to each value in the 'location' column so that heat map in looker focuses only on Toronto\n",
    "df_road_delay_data = df_road_delay_data.withColumn(\"Location\", concat(df_road_delay_data[\"Location\"], lit(\", Ontario\")))\n",
    "\n",
    "df_all_delay_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72fab0b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df_bus\u001b[38;5;241m.\u001b[39mfilter((col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBus_number\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Show the filtered rows\u001b[39;00m\n\u001b[1;32m      4\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39mcount()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "filtered_df = df_bus.filter((col(\"Bus_number\") != 0))\n",
    "\n",
    "# Show the filtered rows\n",
    "filtered_df.count()\n",
    "df_bus.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3b38f",
   "metadata": {},
   "source": [
    "## 4. Loading data into Big Query ðŸ“¥\n",
    "\n",
    "Finally, we push our results to Big Query completing the **Loading** part of the ETL process\n",
    "\n",
    "This requires setting up a temp bucket and We also need to specify the connector jar\n",
    "\n",
    "use `--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar` when submitting a job using the terminal  \n",
    "\n",
    "use `.config(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.11.jar, ./lib/spark-bigquery-with-dependencies_2.12-0.24.0.jar\")` within the `spark = SparkSession.builder` when running in a notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04f748a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "temp_bucket = \"spark_temp_ttc\"\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Saving the data to BigQuery (for overwrite : https://stackoverflow.com/q/72519200/19268172)\n",
    "df_bus.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.bus_delays_table') \\\n",
    "\t.mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "df_subway.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.subway_delays_table') \\\n",
    "\t.mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "df_streetcar.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.streetcar_delays_table') \\\n",
    "\t.mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "df_road_delay_data.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.road_delays_table') \\\n",
    "\t.mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "df_all_delay_data.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.all_delays_table') \\\n",
    "\t.mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2486e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
