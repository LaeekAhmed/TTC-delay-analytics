{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6714fb3",
   "metadata": {},
   "source": [
    "## Spark setup 🌠\n",
    "\n",
    "we will import the required libraries and setup the configs required to start the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c4e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.3\n",
      "/home/shaikh/spark/spark-3.3.3-bin-hadoop3/python/pyspark/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this scirpt is a driver program which creates the SparkContext \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "print(pyspark.__version__)\n",
    "print(pyspark.__file__)\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, round, format_number, initcap\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b81752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ttc-da-instance.us-east4-a.c.ttc-data-analytics.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_etl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0e98182a10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credentials_location = \"/home/shaikh/.google/credentials/ttc-data-analytics-key.json\"\n",
    "\n",
    "# Configure SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('spark_etl') \\\n",
    "    .config(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.11.jar, ./lib/spark-bigquery-with-dependencies_2.12-0.24.0.jar\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure Hadoop Configuration\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "spark\n",
    "# we stored the connector files in the lib/ dir :\n",
    "# !ls lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf1fc3",
   "metadata": {},
   "source": [
    "## Understanding source data columns 📅\n",
    "\n",
    "\n",
    "> Subway\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Date|Date (YYYY/MM/DD)|12/31/2016|\n",
    "|Time|Time (24h clock)|1:59|\n",
    "|Day|Name of the day of the week|Saturday|\n",
    "|Station|TTC subway station name|Rosedale Station|\n",
    "|Code|TTC delay code|MUIS|\n",
    "|Min Delay|Delay (in minutes) to subway service|5|\n",
    "|Min Gap|Time length (in minutes) between trains|9|\n",
    "|Bound|Direction of train dependant on the line|N|\n",
    "|Line|TTC subway line i.e. YU, BD, SHP, and SRT|YU|\n",
    "|Vehicle|TTC train number|5961|\n",
    "\n",
    "we have a **delay code** lookup file  \n",
    "\n",
    "we also have **line code** lookup values : Line 1 Yonge-University (YU), Line 2 Bloor-Danforth (BD), Line 3 Scarborough (SRT), Line 4 Sheppard (SHP)\n",
    "\n",
    "> Bus\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Report Date|The date (YYYY/MM/DD) when the delay-causing incident occurred|6/20/2017|\n",
    "|Route|The number of the bus route|51|\n",
    "|Time|Time (24h clock) when the delay-causing incident occurred|12:35:00 AM|\n",
    "|Day|The name of the day|Monday|\n",
    "|Location|The location of the delay-causing incident|York Mills Station|\n",
    "|Incident|The description of the delay-causing incident|Mechanical|\n",
    "|Min Delay|The delay, in minutes, to the schedule for the following bus|10|\n",
    "|Min Gap|The total scheduled time, in minutes, from the bus ahead of the following bus|20|\n",
    "|Direction|The direction of the bus route where B,b or BW indicates both ways. <br>(On an east west route, it includes both east and west)<br>NB - northbound, SB - southbound, EB - eastbound, WB - westbound|N||\n",
    "|Vehicle|Vehicle number|1057|\n",
    "\n",
    "> Streetcar\n",
    "\n",
    "|Field Name|Description|Example|\n",
    "|---|---|---|\n",
    "|Report Date|The date (YYYY/MM/DD) when the delay-causing incident occurred|6/20/2017|\n",
    "|Route|The number of the streetcar route|51|\n",
    "|Time|Time (24h clock) when the delay-causing incident occurred|12:35:00 AM|\n",
    "|Day|The name of the day|Monday|\n",
    "|Location|The location of the delay-causing incident|York Mills Station|\n",
    "|Incident|The description of the delay-causing incident|Mechanical|\n",
    "|Min Delay|The delay, in minutes, to the schedule for the following streetcar|10|\n",
    "|Min Gap|The total scheduled time, in minutes, from the streetcar ahead of the following streetcar|20|\n",
    "|Direction|The direction of the bus route where B,b or BW indicates both ways.<br>(On an east west route, it includes both east and west)<br>NB - northbound, SB - southbound, EB - eastbound, WB - westbound<br>The direction is not case sensitive|N|\n",
    "|Vehicle|Vehicle number|1057|\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71377b5",
   "metadata": {},
   "source": [
    "## Extracting data from gcs 📤\n",
    "\n",
    "This will be the first part of the ETL process - The **Extraction** (Reading) process\n",
    "\n",
    "to read data from gcs, we follow these steps : [read data from gcs using spark (from boslai's notes)](https://github.com/boisalai/de-zoomcamp-2023/blob/main/week5.md#setup-to-read-from-gcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4f3c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Route: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "\n",
      "Streetcar DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Line: long (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Bound: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n",
      "\n",
      "Subway DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Min Delay: long (nullable = true)\n",
      " |-- Min Gap: long (nullable = true)\n",
      " |-- Bound: string (nullable = true)\n",
      " |-- Line: string (nullable = true)\n",
      " |-- Vehicle: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bus = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/bus_delay_data/*')\n",
    "df_subway = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/subway_delay_data/*')\n",
    "df_streetcar = spark.read.parquet('gs://ttc_data_lake_ttc-data-analytics/streetcar_delay_data/*')\n",
    "\n",
    "print(\"Bus DataFrame Schema:\")\n",
    "df_bus.printSchema()\n",
    "\n",
    "print(\"\\nStreetcar DataFrame Schema:\")\n",
    "df_streetcar.printSchema()\n",
    "\n",
    "print(\"\\nSubway DataFrame Schema:\")\n",
    "df_subway.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab62352",
   "metadata": {},
   "source": [
    "## Transformations 🎇\n",
    "\n",
    "This is the 2nd step of the ETL process, the **Transformation** process. Now we will modify the extracted data so as to make it fit for being used as a table which will later be used to create dashboards.\n",
    "\n",
    "**1. Extra unwanted column**\n",
    "\n",
    "we have an extra column named `index_level_0` in your DataFrame that you didn't have in the original source `.xlsx` file.  \n",
    "This could be due to the index column being converted to a regular column during processing.  \n",
    "\n",
    "In pandas, an index is a special column that serves as a unique identifier for each row in a DataFrame or Series.\n",
    "By default, when you create a DataFrame in pandas, it automatically assigns a numeric index to each row, starting from 0 and incrementing by 1.\n",
    "\n",
    "we will remove this using `df_bus = df_bus.drop(\"__index_level_0__\")`\n",
    "\n",
    "---\n",
    "**2. converting `long` types to `int` as it occupies less memory**  \n",
    "\n",
    "we will use the `.cast(\"int\"))` method\n",
    "\n",
    "---\n",
    "\n",
    "**3. Transforming column values to lower case**\n",
    "\n",
    "The columns `Station` and `Location` have all values in Caps, so we have to change it  \n",
    "\n",
    "we also want to capitalize the first letter of each word in a column while converting the rest of the letters to lowercase, so we can use the `initcap()` method from `pyspark.sql.functions`. using ref : https://stackoverflow.com/a/68370448/19268172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71566fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bus DataFrame Schema:\n",
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Route: integer (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Incident: string (nullable = true)\n",
      " |-- Min Delay: integer (nullable = true)\n",
      " |-- Min Gap: integer (nullable = true)\n",
      " |-- Direction: string (nullable = true)\n",
      " |-- Vehicle: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "|               Date|Route| Time|     Day|            Location|            Incident|Min Delay|Min Gap|Direction|Vehicle|\n",
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "|2022-01-01 00:00:00|  320|02:00|Saturday|    Yonge And Dundas|       General Delay|        0|      0|     null|   8531|\n",
      "|2022-01-01 00:00:00|  325|02:00|Saturday|Overlea And Thorc...|           Diversion|      131|    161|        W|   8658|\n",
      "|2022-01-01 00:00:00|  320|02:00|Saturday|   Yonge And Steeles|Operations - Oper...|       17|     20|        S|      0|\n",
      "|2022-01-01 00:00:00|  320|02:07|Saturday|   Yonge And Steeles|Operations - Oper...|        4|     11|        S|      0|\n",
      "|2022-01-01 00:00:00|  320|02:13|Saturday|   Yonge And Steeles|Operations - Oper...|        4|      8|        S|      0|\n",
      "+-------------------+-----+-----+--------+--------------------+--------------------+---------+-------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. dropping the index column\n",
    "df_bus = df_bus.drop(\"__index_level_0__\")\n",
    "df_streetcar = df_streetcar.drop(\"__index_level_0__\")\n",
    "\n",
    "# 2. Convert common Long Type columns to Integer Type\n",
    "for col_name in [\"Min Delay\", \"Min Gap\", \"Vehicle\"]:\n",
    "    df_bus = df_bus.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    df_subway = df_subway.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    df_streetcar = df_streetcar.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    # ↪ `col(col_name).cast(\"int\")` will be the new value for `col_name`\n",
    "\n",
    "# convert un-common Long Type columns to Integer Type\n",
    "df_streetcar = df_streetcar.withColumn(\"Line\", col(\"Line\").cast(\"int\"))\n",
    "df_bus = df_bus.withColumn(\"Route\", col(\"Route\").cast(\"int\"))\n",
    "\n",
    "# 3. Capitalize the first letter of each word in the \"Location\" column\n",
    "df_bus = df_bus.withColumn(\"Location\", initcap(\"Location\"))\n",
    "df_streetcar = df_streetcar.withColumn(\"Location\", initcap(\"Location\"))\n",
    "df_subway = df_subway.withColumn(\"Station\", initcap(\"Station\"))\n",
    "\n",
    "# check the results\n",
    "print(\"Bus DataFrame Schema:\")\n",
    "df_bus.printSchema()\n",
    "\n",
    "df_bus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5356f",
   "metadata": {},
   "source": [
    "## Continuing Transformations 🛠️\n",
    "\n",
    "3. we use `F.to_date()` built-in Spark function that converts a timestamp to date format (year, month and day only, no hour and minute). on `Date` column\n",
    "---\n",
    "4. we need to rename some columns before merging (union) so that we can distinguish between the types\n",
    "---\n",
    "5. We need to deal with column having `null` values\n",
    "---\n",
    "6. we need to create a union the 3 dfs to create one single df (ie. a table in BQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd893d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident un-common values :  19\n",
      "Incident common values :  11\n",
      "\n",
      "Location un-common values :  14934\n",
      "Location common values :  625\n",
      "\n",
      "Bus DataFrame :\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "|      Date|Bus Route| Time|     Day|            Location|            Incident|Min Delay|Min Gap|Direction|Bus Number|\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "|2022-01-01|      320|02:00|Saturday|    Yonge And Dundas|       General Delay|        0|      0|     null|      8531|\n",
      "|2022-01-01|      325|02:00|Saturday|Overlea And Thorc...|           Diversion|      131|    161|        W|      8658|\n",
      "|2022-01-01|      320|02:00|Saturday|   Yonge And Steeles|Operations - Oper...|       17|     20|        S|         0|\n",
      "|2022-01-01|      320|02:07|Saturday|   Yonge And Steeles|Operations - Oper...|        4|     11|        S|         0|\n",
      "|2022-01-01|      320|02:13|Saturday|   Yonge And Steeles|Operations - Oper...|        4|      8|        S|         0|\n",
      "+----------+---------+-----+--------+--------------------+--------------------+---------+-------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Streetcar DataFrame :\n",
      "+----------+-------------+-----+--------+-------------------+--------------------+---------+-------+-----+--------------+\n",
      "|      Date|Str-car Route| Time|     Day|           Location|            Incident|Min Delay|Min Gap|Bound|Str-car Number|\n",
      "+----------+-------------+-----+--------+-------------------+--------------------+---------+-------+-----+--------------+\n",
      "|2022-01-01|          504|02:21|Saturday|  Broadview Station|Collision - TTC I...|       30|     60|    E|          8333|\n",
      "|2022-01-01|          501|03:22|Saturday|  718 Queen St East|          Operations|       16|     35|    W|          8068|\n",
      "|2022-01-01|          504|03:28|Saturday|  Broadview Station|          Operations|       18|     36|    S|             0|\n",
      "|2022-01-01|          510|03:34|Saturday|      Union Station|          Operations|       30|     60| null|          4406|\n",
      "|2022-01-01|          301|03:39|Saturday|Lakeshore And Tenth|            Security|        5|     25|    W|          8622|\n",
      "+----------+-------------+-----+--------+-------------------+--------------------+---------+-------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Subway DataFrame :\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+-----+----+-------+\n",
      "|      Date| Time|     Day|             Station| Code|Min Delay|Min Gap|Bound|Line|Vehicle|\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+-----+----+-------+\n",
      "|2022-01-01|15:59|Saturday|Lawrence East Sta...| SRDP|        0|      0|    N| SRT|   3023|\n",
      "|2022-01-01|02:23|Saturday|  Spadina Bd Station| MUIS|        0|      0| null|  BD|      0|\n",
      "|2022-01-01|22:00|Saturday|Kennedy Srt Stati...|  MRO|        0|      0| null| SRT|      0|\n",
      "|2022-01-01|02:28|Saturday|  Vaughan Mc Station| MUIS|        0|      0| null|  YU|      0|\n",
      "|2022-01-01|02:34|Saturday|    Eglinton Station|MUATC|        0|      0|    S|  YU|   5981|\n",
      "+----------+-----+--------+--------------------+-----+---------+-------+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 3. modifying the DataFrame inside the loop won't affect the original DataFrames unless we reassign them.\n",
    "dataframes = [df_bus, df_streetcar, df_subway]\n",
    "\n",
    "for i in range(len(dataframes)):\n",
    "    dataframes[i] = dataframes[i].withColumn('Date', F.to_date(dataframes[i].Date))\n",
    "\n",
    "# Re-assign the modified DataFrames to their original variable names\n",
    "df_bus, df_streetcar, df_subway = dataframes\n",
    "\n",
    "\n",
    "# 4. Rename some columns\n",
    "df_bus = df_bus \\\n",
    "    .withColumnRenamed('Route', 'Bus Route') \\\n",
    "    .withColumnRenamed('Vehicle', 'Bus Number')\n",
    "\n",
    "df_streetcar = df_streetcar \\\n",
    "    .withColumnRenamed('Line', 'Str-car Route') \\\n",
    "    .withColumnRenamed('Vehicle', 'Str-car Number')\n",
    "\n",
    "\n",
    "# check if the columns contain the same values\n",
    "\n",
    "# Find difference in values in df_bus.Incident & df_streetcar.Incident\n",
    "count1 = df_bus.select(\"Incident\") \\\n",
    "    .exceptAll(df_streetcar.select(\"Incident\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "count2 = df_streetcar.select(\"Incident\") \\\n",
    "    .exceptAll(df_bus.select(\"Incident\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "print(\"Incident un-common values : \",count1+count2)\n",
    "\n",
    "print(\"Incident common values : \",df_bus.select(\"Incident\") \\\n",
    "    .intersect(df_streetcar.select(\"Incident\")) \\\n",
    "    .count())\n",
    "\n",
    "# Find difference in values in df_bus.Location & df_streetcar.Location\n",
    "count1 = df_bus.select(\"Location\") \\\n",
    "    .exceptAll(df_streetcar.select(\"Location\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "count2 = df_streetcar.select(\"Location\") \\\n",
    "    .exceptAll(df_bus.select(\"Location\")) \\\n",
    "    .distinct().count()\n",
    "\n",
    "print(\"\\nLocation un-common values : \",count1+count2)\n",
    "\n",
    "print(\"Location common values : \",df_bus.select(\"Location\") \\\n",
    "    .intersect(df_streetcar.select(\"Location\")) \\\n",
    "    .count())\n",
    "\n",
    "# check the results\n",
    "print(\"\\nBus DataFrame :\")\n",
    "df_bus.show(5)\n",
    "\n",
    "print(\"\\nStreetcar DataFrame :\")\n",
    "df_streetcar.show(5)\n",
    "\n",
    "print(\"\\nSubway DataFrame :\")\n",
    "df_subway.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab3b38f",
   "metadata": {},
   "source": [
    "## Loading data into Big Query 📥\n",
    "\n",
    "Finally, we push our results to Big Query completing the **Loading** part of the ETL process\n",
    "\n",
    "This requires setting up a temp bucket and We also need to specify the connector jar\n",
    "\n",
    "use `--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar` when submitting a job using the terminal  \n",
    "\n",
    "use `.config(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.11.jar, ./lib/spark-bigquery-with-dependencies_2.12-0.24.0.jar\")` within the `spark = SparkSession.builder` when running in a notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04f748a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used by the connector.\n",
    "temp_bucket = \"spark_temp_ttc\"\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "df_bus.write.format('bigquery') \\\n",
    "    .option('table', 'ttc_delays_data.ttc_delays_table') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2486e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
